{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import argparse\n",
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "INTERNAL_COLOR = '#0072BB'\n",
    "EXTERNAL_COLOR = '#FF9F40'\n",
    "ERROR_COLOR = '#FF0800'\n",
    "RESOURCE_COLOR = '#2ECC71'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error(error, error_obj, r, url, visited, error_codes):\n",
    "    error = str(error_obj) if error else r.status_code\n",
    "    visited.add(url)\n",
    "    error_codes[url] = error\n",
    "    print(f'{error} ERROR while visiting {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url, visit_external):\n",
    "    visited = set()\n",
    "    edges = set()\n",
    "    resource_pages = set()\n",
    "    error_codes = dict()\n",
    "    redirect_target_url = dict()\n",
    "\n",
    "    head = requests.head(url, timeout=2)\n",
    "    site_url = head.url\n",
    "    redirect_target_url[url] = site_url\n",
    "\n",
    "    to_visit = deque()\n",
    "    to_visit.append((site_url, None))\n",
    "\n",
    "    while to_visit:\n",
    "        url, from_url = to_visit.pop()\n",
    "\n",
    "        print('Visiting', url, 'from', from_url)\n",
    "\n",
    "        error = False\n",
    "        error_obj = None\n",
    "        try:\n",
    "            page = requests.get(url, timeout=2)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            error = True\n",
    "            error_obj = e\n",
    "\n",
    "        if error or not page:\n",
    "            handle_error(error, error_obj, page, url, visited, error_codes)\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        internal_links = set()\n",
    "        external_links = set()\n",
    "\n",
    "        # Handle <base> tags\n",
    "        base_url = soup.find('base')\n",
    "        base_url = '' if base_url is None else base_url.get('href', '')\n",
    "\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            link_url = link['href']\n",
    "        \n",
    "            if link_url.startswith('mailto:'):\n",
    "                continue\n",
    "            \n",
    "            # Resolve relative paths\n",
    "            if not link_url.startswith('http'):\n",
    "                link_url = urllib.parse.urljoin(url, urllib.parse.urljoin(base_url, link_url))\n",
    "\n",
    "            # Remove queries/fragments from internal links\n",
    "            if link_url.startswith(site_url):\n",
    "                link_url = urllib.parse.urljoin(link_url, urllib.parse.urlparse(link_url).path)\n",
    "\n",
    "            # Load where we know that link_url will be redirected\n",
    "            if link_url in redirect_target_url:\n",
    "                link_url = redirect_target_url[link_url]\n",
    "\n",
    "            if link_url not in visited and (visit_external or link_url.startswith(site_url)):\n",
    "                is_html = False\n",
    "                error = False\n",
    "                error_obj = None\n",
    "\n",
    "                try:\n",
    "                    head = requests.head(link_url, timeout=2)\n",
    "                    if head and 'html' in head.headers.get('content-type', ''):\n",
    "                        is_html = True\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    error = True\n",
    "                    error_obj = e\n",
    "\n",
    "                if error or not head:\n",
    "                    handle_error(error, error_obj, head, link_url, visited, error_codes)\n",
    "                    edges.add((url, link_url))\n",
    "                    continue\n",
    "\n",
    "                redirect_target_url[link_url] = head.url\n",
    "                link_url = head.url\n",
    "                visited.add(link_url)\n",
    "\n",
    "                if link_url.startswith(site_url):\n",
    "                    if is_html:\n",
    "                        to_visit.append((head.url, url))\n",
    "                    else:\n",
    "                        resource_pages.add(link_url)\n",
    "            \n",
    "            edges.add((url, link_url))\n",
    "    \n",
    "    return edges, error_codes, resource_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_info(nodes, error_codes, resource_pages, args):\n",
    "    node_info = []\n",
    "    for node in nodes:\n",
    "        if node in error_codes:\n",
    "            node_info.append(f'Error: {error_codes[node]}')\n",
    "        elif node in resource_pages:\n",
    "            node_info.append('resource')\n",
    "        elif node.startswith(args.site_url):\n",
    "            node_info.append('internal')\n",
    "        else:\n",
    "            node_info.append('external')\n",
    "    return node_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(edges, error_codes, resource_pages, args):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    G.add_edges_from(edges) \n",
    "    pr1 = nx.pagerank(G)#, alpha=1, max_iter=500, tol=1e-15)\n",
    "    \n",
    "    F = nx.DiGraph()\n",
    "    F.add_edges_from(edges)\n",
    "    S = nx.convert_node_labels_to_integers(F)\n",
    "    pr2 = nx.pagerank(S)#, alpha=1, max_iter=500, tol=1e-15)\n",
    "    with open(\"pagerank_2.txt\", 'w') as f: \n",
    "        for key, value in pr2.items(): \n",
    "            f.write('%s:%s\\n' % (key,value))\n",
    "    \n",
    "    with open(\"pagerank.txt\", 'w') as f: \n",
    "        for key, value in pr1.items(): \n",
    "            f.write('%s:%s\\n' % (key,value))\n",
    "    \n",
    "    with open(\"pagerank_1.txt\", 'w') as f: \n",
    "        for key, value in pr1.items(): \n",
    "            f.write('%s\\n' % (value))\n",
    "            \n",
    "    if args.save_txt is not None or args.save_npz is not None:\n",
    "        nodes = list(G.nodes())\n",
    "        adj_matrix = nx.to_numpy_matrix(G, nodelist=nodes, dtype=int)\n",
    "\n",
    "        if args.save_npz is not None:\n",
    "            base_fname = args.save_npz.replace('.npz', '')\n",
    "            scipy.sparse.save_npz(args.save_npz, scipy.sparse.coo_matrix(adj_matrix))\n",
    "        else:\n",
    "            base_fname = args.save_txt.replace('.txt', '')\n",
    "            np.savetxt(args.save_txt, adj_matrix, fmt='%d')\n",
    "\n",
    "        node_info = get_node_info(nodes, error_codes, resource_pages, args)\n",
    "        with open(base_fname + '_nodes.txt', 'w') as f:\n",
    "            f.write('\\n'.join([nodes[i] + '\\t' + node_info[i] for i in range(len(nodes))]))\n",
    "\n",
    "    net = Network(width=args.width, height=args.height) #directed = True - arrows\n",
    "    net.from_nx(G)\n",
    "\n",
    "    \n",
    "    net.show_buttons()\n",
    "    if args.options is not None:\n",
    "        try:\n",
    "            with open(args.options, 'r') as f:\n",
    "                net.set_options(f.read())\n",
    "        except FileNotFoundError as e:\n",
    "            print('Error: options file', args.options, 'not found.')\n",
    "        except Exception as e:\n",
    "            print('Error applying options:', e)\n",
    "    \n",
    "    for node in net.nodes:\n",
    "        node['size'] = 15\n",
    "        node['label'] = ''\n",
    "        #node['physics'] = \"False\"\n",
    "        \n",
    "        if node['id'].startswith(args.site_url):\n",
    "            node['color'] = INTERNAL_COLOR\n",
    "            if node['id'] in resource_pages:\n",
    "                node['color'] = RESOURCE_COLOR\n",
    "        else:\n",
    "            node['color'] = EXTERNAL_COLOR\n",
    "        \n",
    "        if node['id'] in error_codes:\n",
    "            node['title'] = f'{error_codes[node[\"id\"]]} Error: <a href=\"{node[\"id\"]}\">{node[\"id\"]}</a>'\n",
    "            node['color'] = ERROR_COLOR\n",
    "        else:\n",
    "            node['title'] = f'<a href=\"{node[\"id\"]}\">{node[\"id\"]}</a>'\n",
    "\n",
    "    #net.toggle_physics(False)\n",
    "    net.save_graph(args.vis_file)\n",
    "    #net.show(args.vis_file, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1044256810.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    args.site_url = #put here any site you want to scrap\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Visualize the link graph of a website.')\n",
    "    parser.add_argument('site_url', type=str, help='the base URL of the website', nargs='?', default='')\n",
    "    default = 'site.html'\n",
    "    parser.add_argument('--vis-file', type=str, help='filename in which to save HTML graph visualization (default: ' + default + ')', default=default)\n",
    "    default = 'crawl.pickle'\n",
    "    parser.add_argument('--data-file', type=str, help='filename in which to save crawled graph data (default: ' + default + ')', default=default)\n",
    "    default = 1500\n",
    "    parser.add_argument('--width', type=int, help='width of graph visualization in pixels (default: ' + str(default) + ')', default=default)\n",
    "    default = 1200\n",
    "    parser.add_argument('--height', type=int, help='height of graph visualization in pixels (default: ' + str(default) + ')', default=default)\n",
    "    parser.add_argument('--visit-external', action='store_true', help='detect broken external links (slower)')\n",
    "    parser.add_argument('--show-buttons', action='store_true', help='show visualization settings UI')\n",
    "    parser.add_argument('--options', type=str, help='file with drawing options (use --show-buttons to configure, then generate options)')\n",
    "    parser.add_argument('--from-data-file', type=str, help='create visualization from given data file', default=None)\n",
    "    parser.add_argument('--force', action='store_true', help='override warnings about base URL')\n",
    "    parser.add_argument('--save-txt', type=str, nargs='?', help='filename in which to save adjacency matrix (if no argument, uses adj_matrix.txt). Also saves node labels to [filename]_nodes.txt', const='adj_matrix.txt', default=None)\n",
    "    parser.add_argument('--save-npz', type=str, nargs='?', help='filename in which to save sparse adjacency matrix (if no argument, uses adj_matrix.npz). Also saves node labels to [filename]_nodes.txt',  const='adj_matrix.npz', default=None)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = vars(parser.parse_args(args=[]))\n",
    "    args.site_url = #put here any site you want to scrap\n",
    "    if args.from_data_file is None:\n",
    "        if not args.site_url.endswith('/'):\n",
    "            if not args.force:\n",
    "                print('Warning: no trailing slash on site_url (may get duplicate homepage node). If you really don\\'t want the trailing slash, run with --force')\n",
    "                exit(1)\n",
    "\n",
    "        if not args.site_url.startswith('https'):\n",
    "            if not args.force:\n",
    "                print('Warning: not using https. If you really want to use http, run with --force')\n",
    "                exit(1)\n",
    "        \n",
    "        edges, error_codes, resource_pages = crawl(args.site_url, args.visit_external)\n",
    "        print('Crawl complete.')\n",
    "\n",
    "        with open(args.data_file, 'wb') as f:\n",
    "            pickle.dump((edges, error_codes, resource_pages, args.site_url), f)\n",
    "            print(f'Saved crawl data to {args.data_file}')\n",
    "    else:\n",
    "        with open(args.from_data_file, 'rb') as f:\n",
    "            edges, error_codes, resource_pages, site_url = pickle.load(f)\n",
    "            args.site_url = site_url\n",
    "\n",
    "    cw = csv.writer(open(\"hello.csv\",'w'))\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(list(edges), columns=[\"colummn1\",\"colummn2\"])\n",
    "    df.to_csv('list.csv', index=False)\n",
    "\n",
    "    #print(list(edges))\n",
    "    #cw.writerow(list(edges))\n",
    "   \n",
    "    #print(edges)\n",
    "    visualize(edges, error_codes, resource_pages, args)\n",
    "    print('Saved graph to', args.vis_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(edges), columns=[\"colummn1\",\"colummn2\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = nx.DiGraph()\n",
    "F.add_edges_from(edges)\n",
    "\n",
    "S = nx.convert_node_labels_to_integers(F)\n",
    "nodes_F = list(S.nodes())\n",
    "edges_F = list(S.edges())\n",
    "\n",
    "#print(edges)\n",
    "df1 = pd.DataFrame(list(S.edges()), columns=[\"colummn1\",\"colummn2\"])\n",
    "df1.to_csv('list_integer.csv', index=False)\n",
    "#print(df1)\n",
    "\n",
    "pr1 = nx.pagerank(S)#, alpha=1, max_iter=500, tol=1e-15)\n",
    "for key, value in pr1.items(): \n",
    "    print('%s:%s\\n' % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1_sorted = sorted(pr1.items(), key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(data)\n",
    "G = nx.convert_node_labels_to_integers(G)\n",
    "print(len(G.edges()))\n",
    "simple_pagerank = nx.pagerank(G, alpha=1, max_iter=500, tol=1e-15)\n",
    "pr2 = nx.pagerank(G)#, alpha=1, max_iter=500, tol=1e-15)\n",
    "for key, value in pr2.items(): \n",
    "    print('%s:%s\\n' % (key,value))\n",
    "pr2_sorted = sorted(pr2.items(), key=lambda x:x[1], reverse = True)\n",
    "print(pr2_sorted)\n",
    "hubs, authorities = nx.hits(G)\n",
    "for key, value in authorities.items():\n",
    "    print('%s:%s' % (key,value))\n",
    "hubs_sorted = sorted(hubs.items(), key=lambda x:x[1], reverse = True)\n",
    "print(hubs_sorted)\n",
    "authorities_sorted = sorted(authorities.items(), key=lambda x:x[1], reverse = True)\n",
    "print(authorities_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = net.Network(notebook = True, width = 1000, height = 800, directed = True)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "[G.add_node(k) for k in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]]\n",
    "G.add_edges_from([('1','2'),('1','3'), ('1','6'),\n",
    "                  ('2','4'),\n",
    "                  ('3','1'),('3','2'),('3','4'),\n",
    "                  ('4','5'),('4','6'),\n",
    "                  ('5','4'),('5','6'),\n",
    "                  ('6','3'),('6','5')\n",
    "                 ])\n",
    "\n",
    "simple_pagerank = nx.pagerank(G, alpha=1, max_iter=500, tol=1e-15)\n",
    "\n",
    "g.from_nx(G)\n",
    "g.show(\"example.html\")\n",
    "\n",
    "simple_pagerank = nx.pagerank(G)\n",
    "\n",
    "print(simple_pagerank)\n",
    "\n",
    "h, a = nx.hits(G)\n",
    "print(h)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
